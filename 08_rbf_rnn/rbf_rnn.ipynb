{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basis Functions a Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minule jsme si ukazovali a zkoušeli si naprogramovat jednoduché neuronové sítě. Dneska se podíváme na složitější struktury neuronových sítí Radial Basis Functions a rekurentní neuronové sítě.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Functions (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujeme si nyní RBF síť. Implementace je jednoduchá - stačí naimplementovat třídu podobnou té níže, kde vlastní výpočet je schován do funkce *predict*, trénování klasicky do funkce *fit* a to nejdůležitější, výpočet RBF aktivace, pak do metody *calculate_activation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork():\n",
    "    def __init__(self, input_dim, num_centers, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_centers = num_centers\n",
    "        self.centers = [np.random.uniform(-1, 1, input_dim) for i in range(num_centers)]\n",
    "        self.beta = 1\n",
    "        self.weights = None\n",
    "        \n",
    "        \n",
    "    # Compute values of neurons in the hidden layer\n",
    "    def calculate_activation(self, data):\n",
    "        hidden_layer_values = np.zeros((data.shape[0], self.num_centers), float)\n",
    "        for c_idx, c in enumerate(self.centers):\n",
    "            for x_idx, x in enumerate(data):\n",
    "                hidden_layer_values[x_idx, c_idx] = self.activation_fcn(c, x)\n",
    "                \n",
    "        return hidden_layer_values\n",
    "    \n",
    "    \n",
    "    # Value of the activation function (for the hidden - RBF - layer)\n",
    "    def activation_fcn(self, centers, data):\n",
    "        return np.exp(-self.beta * np.linalg.norm(centers-data)**2)\n",
    "    \n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        # PART 1 - Set centers for the first layer\n",
    "        \n",
    "        # Choose random values (from the dataset) for the initial centers\n",
    "        random_idx = np.random.permutation(data.shape[0])[:self.num_centers]\n",
    "        self.centers = [data[i,:] for i in random_idx]\n",
    "        \n",
    "        # PART 2 - Set weights for the second layer\n",
    "         \n",
    "        # Calculate activation on the hidden layer\n",
    "        hidden_layer_values = self.calculate_activation(data)\n",
    "         \n",
    "        # Compare the real and the predicted outputs and update our weights\n",
    "        # (Pseudoinverse matrix basically corresponds to the formula for linear\n",
    "        # regression we would use for training of the weights.)\n",
    "        self.weights = np.dot(np.linalg.pinv(hidden_layer_values), labels)\n",
    "          \n",
    "          \n",
    "    def predict(self, data):\n",
    "        hidden_layer_values = self.calculate_activation(data)\n",
    "        labels = np.dot(hidden_layer_values, self.weights)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusíme si naši RBF síť pustit na našem oblíbeném datasetu Iris. Načteme si data a labely, do které třídy data patří. Protože budeme dělat klasifikaci, je vhodné si labely převést na one-hot-encoding. Následně data rozdělíme na trénovací a testovací množinu, abychom mohli zvlášť data trénovat a zvlášť data testovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8947368421052632\n",
      "Accuracy: 0.9473684210526315\n",
      "Accuracy: 0.9736842105263158\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.8421052631578947\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.9210526315789473\n",
      "Accuracy: 0.9210526315789473\n",
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "# Format the input using one-hot-encoding\n",
    "# -> Create a ColumnTransformer that will apply OneHotEncoder to the first column\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"onehot\", OneHotEncoder(), [0])\n",
    "    ],\n",
    "    remainder=\"passthrough\" # This will pass the rest of the columns through without any transformation\n",
    ")\n",
    "\n",
    "# Apply the ColumnTransformer to y_train and y_test\n",
    "y_train_onehot = column_transformer.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = column_transformer.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Train and test the network multiple times\n",
    "for _ in range(10):\n",
    "    rbf = RBFNetwork(4, 10, 3)\n",
    "    rbf.fit(x_train, y_train_onehot)\n",
    "    predicted = rbf.predict(x_test)\n",
    "    y_pred = np.argmax(predicted, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Když si trénování a predikci spustíme několikrát, vidíme, že accuracy je v každém běhu dosti různá, což je způsobeno tím, že centroidy a betu zde nastavujeme náhodně, přestože jsme si říkali, že pozice centroidů se dá trénovat pomocí algoritmu k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Úkol na cvičení\n",
    "\n",
    "Zkuste si naimplementovat algoritmus k-means pro inicializaci středů vstupních neuronů a zlepšit tím výstup sítě výše.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekurentní neuronové sítě (RNN)\n",
    "\n",
    "Rekurentní neuronová síť je síť, která navíc ke svému vstupu ještě bere jako další vstup svůj výstup z předchozího kroku. Proto výstupy z předchozích výpočtů mohou ovlivňovat výpočty následující, což se může hodit například u generování časových řad nebo textu. Nejprve se podíváme, jak by zhruba mohla vypadat implementace jednoduché RNN, kdybychom si ji psali celou sami. \n",
    "\n",
    "Vytvoříme si nejprve jednoduché věty a budeme chtít, aby nám naše sít uměla predikovat, zda je daná věta pozitivní, nebo negativní. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  \"good\": True,\n",
    "  \"bad\": False,\n",
    "  \"happy\": True,\n",
    "  \"sad\": False,\n",
    "  \"not good\": False,\n",
    "  \"not bad\": True,\n",
    "  \"not happy\": False,\n",
    "  \"not sad\": True,\n",
    "  \"very good\": True,\n",
    "  \"very bad\": False,\n",
    "  \"very happy\": True,\n",
    "  \"very sad\": False,\n",
    "  \"i am happy\": True,\n",
    "  \"this is good\": True,\n",
    "  \"i am bad\": False,\n",
    "  \"this is bad\": False,\n",
    "  \"i am sad\": False,\n",
    "  \"this is sad\": False,\n",
    "  \"i am not happy\": False,\n",
    "  \"this is not good\": False,\n",
    "  \"i am not bad\": True,\n",
    "  \"this is not sad\": True,\n",
    "  \"i am very happy\": True,\n",
    "  \"this is very good\": True,\n",
    "  \"i am very bad\": False,\n",
    "  \"this is very sad\": False,\n",
    "  \"this is very happy\": True,\n",
    "  \"i am good not bad\": True,\n",
    "  \"this is good not bad\": True,\n",
    "  \"i am bad not good\": False,\n",
    "  \"i am good and happy\": True,\n",
    "  \"this is not good and not happy\": False,\n",
    "  \"i am not at all good\": False,\n",
    "  \"i am not at all bad\": True,\n",
    "  \"i am not at all happy\": False,\n",
    "  \"this is not at all sad\": True,\n",
    "  \"this is not at all happy\": False,\n",
    "  \"i am good right now\": True,\n",
    "  \"i am bad right now\": False,\n",
    "  \"this is bad right now\": False,\n",
    "  \"i am sad right now\": False,\n",
    "  \"i was good earlier\": True,\n",
    "  \"i was happy earlier\": True,\n",
    "  \"i was bad earlier\": False,\n",
    "  \"i was sad earlier\": False,\n",
    "  \"i am very bad right now\": False,\n",
    "  \"this is very good right now\": True,\n",
    "  \"this is very sad right now\": False,\n",
    "  \"this was bad earlier\": False,\n",
    "  \"this was very good earlier\": True,\n",
    "  \"this was very bad earlier\": False,\n",
    "  \"this was very happy earlier\": True,\n",
    "  \"this was very sad earlier\": False,\n",
    "  \"i was good and not bad earlier\": True,\n",
    "  \"i was not good and not happy earlier\": False,\n",
    "  \"i am not at all bad or sad right now\": True,\n",
    "  \"i am not at all good or happy right now\": False,\n",
    "  \"this was not happy and not good earlier\": False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  \"this is happy\": True,\n",
    "  \"i am good\": True,\n",
    "  \"this is not happy\": False,\n",
    "  \"i am not good\": False,\n",
    "  \"this is not bad\": True,\n",
    "  \"i am not sad\": True,\n",
    "  \"i am very good\": True,\n",
    "  \"this is very bad\": False,\n",
    "  \"i am very sad\": False,\n",
    "  \"this is bad not good\": False,\n",
    "  \"this is good and happy\": True,\n",
    "  \"i am not good and not happy\": False,\n",
    "  \"i am not at all sad\": True,\n",
    "  \"this is not at all good\": False,\n",
    "  \"this is not at all bad\": True,\n",
    "  \"this is good right now\": True,\n",
    "  \"this is sad right now\": False,\n",
    "  \"this is very bad right now\": False,\n",
    "  \"this was good earlier\": True,\n",
    "  \"i was not happy and not good earlier\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve je potřeba udělat nějaký preprocessing vět. Tím se myslí převést text do číselné reprezentace. To uděláme třeba tak, že najdeme všechna unikátní slova, očíslujeme je, a každé slovo pak nahradíme jeho číslem. Následně uděláme one-hot-encoding každého slova (a to potom bude mít shape (vocab_size, 1)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['or', 'happy', 'this', 'now', 'is', 'right', 'earlier', 'bad', 'sad', 'not', 'am', 'i', 'very', 'was', 'good', 'at', 'and', 'all']\n",
      "Unique words: 18\n",
      "1\n",
      "or\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary containing every word in the data\n",
    "vocab = list(set(w for text in train_data.keys() for w in text.split()))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "print(\"Unique words: \" + str(vocab_size))\n",
    "\n",
    "# Replace every word by its index\n",
    "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
    "idx_to_word = { i: w for i, w in enumerate(vocab) }\n",
    "print(word_to_idx[\"happy\"]) \n",
    "print(idx_to_word[0]) \n",
    "\n",
    "\n",
    "# Convert to one-hot-encoding\n",
    "def create_inputs(text):\n",
    "    inputs = []\n",
    "    for w in text.split():\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[w]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní si napíšeme samotnou RNN. Informace z předchozího kroku se ukládá v RNN buňce do speciální proměnné - tzv. skrytého stavu, který nám pak bude ovlivňovat další výstupy a bude se s každým novým vstupem v každém kroku aktualizovat. Aktuální skrytý stav se počítá podle předchozího skrytého stavu a aktuálního vstupu. Výstup je spočítán pomocí aktuálního skrytého stavu. Pro každý krok se používají stejné 3 váhové matice: pro spoje z aktuálních vstupů do aktuálních skrytých, pro spoje z předchozích skrytých do aktuálních skrytých a pro spoje z aktuálních skrytých do výstupů. Zároveň potřebujeme bias pro spočtení skrytého stavu a další pro spočtení výstupu. Pomocí aktivační funkce tanh a dosazení hodnot do rovnice vypočteme výstup a update skrytých stavů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights - the division is used for lowering the variance\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        \n",
    "    # Processes whole sequence (not just one token / piece)\n",
    "    def forward(self, inputs):\n",
    "        # Variable for remembering the previous hidden state value\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        # Gradual update of the hidden state\n",
    "        for x in inputs:\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "\n",
    "        # Output vector computation\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50000111]\n",
      " [0.49999889]]\n"
     ]
    }
   ],
   "source": [
    "# Let us define function softmax for mapping values to interval [0,1]\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "\n",
    "# RNN initialization\n",
    "inputs = create_inputs(\"i am very good\")\n",
    "rnn = RNN(vocab_size, 2)\n",
    "y, h = rnn.forward(inputs)\n",
    "probs = softmax(y)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že síť nám sice nějak funguje, ale není moc užitečná. Problém je, že nijak netrénujeme váhy. K tomu je potřeba si definovat ztrátovou (loss) funkci. Použijeme cross-entropy loss, která se v tomto případě spočítá pro každý vstup v podstatě jako minus logaritmus pravděpodobnosti, s jakou predikuje náš model tu opravdovou třídu (label) daného vstupu. Zároveň je potřeba dopsat zpětnou propagaci chyby, aby se síť mohla učit ze svých chyb a updatovat si váhy a skryté stavy. To je v podstatě jen derivace tanh, dosazení do vzorečků a použití řetězového pravidla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights - the division is used for lowering the variance\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "     \n",
    "        \n",
    "    # Processes whole sequence (not just one token / piece)\n",
    "    def forward(self, inputs):\n",
    "        # Variable for remembering the previous hidden state value\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # Gradual update of the hidden state\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # Output vector computation\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    \n",
    "    def backprop(self, probs, target, learning_rate=2e-2):\n",
    "        # Compute derivation of the loss function on the output (dL/dy)\n",
    "        # (For now just believe me this is the right derivation of L(softmax(y)) w.r.t. y.)\n",
    "        d_y = probs\n",
    "        d_y[target] -= 1\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # Compute derivation of the loss function for output weights (dL/dWhy) and bias (dL/dby)\n",
    "        d_Why = np.dot(d_y, self.last_hs[n].T)\n",
    "        d_by = d_y\n",
    "\n",
    "        # Initialize zero matrices and vector for comuting dL/dWhh, dL/dWxh, and dL/dbh\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Compute dL/dh for the last value of h\n",
    "        d_h = np.dot(self.Why.T, d_y)\n",
    "\n",
    "        # Backpropagate the loss back in time by substitution into the equations\n",
    "        for t in reversed(range(n)):\n",
    "            # Auxiliary value: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "\n",
    "            # dL/dbh = dL/dh * (1 - h^2)\n",
    "            d_bh += temp\n",
    "\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += np.dot(temp, self.last_hs[t].T)\n",
    "\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += np.dot(temp, self.last_inputs[t].T)\n",
    "\n",
    "            # dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = np.dot(self.Whh, temp)\n",
    "\n",
    "        # To prevent too large gradients, we restrict / clip the values to interval [-1, 1]\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "                  np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update of weights and biases using gradient descent\n",
    "        self.Whh -= learning_rate * d_Whh\n",
    "        self.Wxh -= learning_rate * d_Wxh\n",
    "        self.Why -= learning_rate * d_Why\n",
    "        self.bh -= learning_rate * d_bh\n",
    "        self.by -= learning_rate * d_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0\n",
      "Train:\tLoss 0.693 | Accuracy: 0.552\n",
      "Test:\tLoss 0.696 | Accuracy: 0.500\n",
      "--- Epoch 100\n",
      "Train:\tLoss 0.688 | Accuracy: 0.534\n",
      "Test:\tLoss 0.698 | Accuracy: 0.500\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.672 | Accuracy: 0.638\n",
      "Test:\tLoss 0.721 | Accuracy: 0.600\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.135 | Accuracy: 0.966\n",
      "Test:\tLoss 0.146 | Accuracy: 1.000\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.015 | Accuracy: 1.000\n",
      "Test:\tLoss 0.019 | Accuracy: 1.000\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.005 | Accuracy: 1.000\n",
      "Test:\tLoss 0.007 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.004 | Accuracy: 1.000\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.003 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "def run_model(data, train):\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    correct_answers = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = create_inputs(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # Forward pass\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        correct_answers += int(np.argmax(probs) == target)\n",
    "\n",
    "        if train:\n",
    "            # Backward pass\n",
    "            rnn.backprop(probs, target)\n",
    "\n",
    "    return loss / len(data), correct_answers / len(data)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "for epoch in range(1001):\n",
    "    train_loss, train_accuracy = run_model(train_data, train=True)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"--- Epoch %d\" % (epoch))\n",
    "        print(\"Train:\\tLoss %.3f | Accuracy: %.3f\" % (train_loss.item(), train_accuracy))\n",
    "\n",
    "        test_loss, test_accuracy = run_model(test_data, train=False)\n",
    "        print(\"Test:\\tLoss %.3f | Accuracy: %.3f\" % (test_loss.item(), test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sekvenční klasifikace pomocí LSTM\n",
    "\n",
    "Nyní, když chápeme, jak taková základní RNN funguje, zkusíme se podívat na složitější druh RNN - LSTM sítě. Tyto sítě mají uvnitř sebe paměťovou buňku a navíc i mechanizmus, který řídí, jakou informaci si buňka pamatuje a jakou zapomíná. Zkusíme se na ni lépe podívat v následujícím příkladu sekvenční klasifikace. \n",
    "\n",
    "Sekvenční klasifikace je prediktivní modelovací problém, kdy máme na vstupu nějakou sekvenci v prostoru nebo čase a cílem je předpovědět kategorii této sekvence (jako tomu bylo třeba i v předchozím příkladu). Složitost tohoto problému spočívá v tom, že jednotlivé sekvence mohou mít různou délku nebo mohou být složeny z rozsáhlého slovníku vstupních hodnot a mohou vyžadovat, aby se model naučil nějaké dlouhodobé závislosti nebo kontext mezi vstupními sekvencemi.\n",
    "\n",
    "Zkusíme se tedy podívat na příklad sekvenční klasifikace pomocí LSTM na IMDB datasetu, což je dataset, který obsahuje slovní popis recenzí 50K filmů a následně klasifikaci, jestli byla recenze pozitivní nebo negativní v poměru zhruba 1:1.\n",
    "\n",
    "Problém je, že slovní popis je nějak potřeba převést na číselnou reprezentaci. Naštěstí funkce `imdb.load_data` umí načíst data tak, že rovnou slova nahradí čísly a rozdělí je na train a test množiny v poměru 1:1. Navíc data načteme tak, že necháme jen prvních `top_words` nejčastějších slov a zbytek nahradíme 0. Dále je potřeba zkrátit nebo doplnit vstupní sekvence pro modelování tak, aby byly všechny stejně dlouhé - délku nastavíme na `max_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us fix the random seed for the sake of reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=top_words)\n",
    "\n",
    "max_length = 500\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní máme připravená data a můžeme si definovat a natrénovat model.\n",
    " - První vrstva je `Embedding`, která používá vektory délky 32 pro každé slovo. \n",
    " - Další vrstva je `LSTM` vrstva, která obsahuje 100 paměťových jednotek (neuronů). \n",
    " - Na závěr použijeme `Dense` výstupní vrstvu s jedním neuronem a aktivační funkcí sigmoid k vytvoření predikcí 0 nebo 1, protože se jedná o klasifikační úlohu.\n",
    "\n",
    "Problém modelu je, že se velice snadno overfittuje na na daná trénovací data. Proto se se používají ještě vrstvy `Dropout`, které spočívají v tom, že během trénování se náhodně vynechávají některé vstupy do další vrstvy. Tím se simuluje velký počet sítí s odlišnou strukturou a uzly jsou pak robustnější a omezuje se tím pádem overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 33s 72ms/step - loss: 0.5115 - accuracy: 0.7307\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4196 - accuracy: 0.8258\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2764 - accuracy: 0.8927\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "embedding_vector_length = 32\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.embeddings.Embedding(top_words, embedding_vector_length, input_length=max_length))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.LSTM(100))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na závěr zkusíme predikovat výstupy na testovacích datech a podívat se, jak je model dobrý. Můžete si třeba zkusit pustit trénování modelu s droupoutem a bez něj a podívat se, jak se budou lišit výsledné accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.29%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generování textu znak po znaku \n",
    "\n",
    "Nyní se podíváme na jiný druh problému - budeme generovat text znak po znaku, neboli natrénujeme jazykový model tak, že když mu pak dáme sekvenci znaků, tak nám model bude schopný předpovědět další znak. Jako trénovací množinu použijeme texty Nietzscheho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Example script to generate text from Nietzsche\"s writings.\n",
    "    At least 20 epochs are required before the generated text\n",
    "    starts to sound coherent.\n",
    "    It is recommended to run this script on GPU, as recurrent\n",
    "    networks are quite computationally intensive.\n",
    "    If you try this script on a new data, make sure your corpus\n",
    "    has at least ~100k characters. ~1M is better.\n",
    "\"\"\"\n",
    "\n",
    "# Load the input data\n",
    "path = tf.keras.utils.get_file(\"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print(\"corpus length:\", len(text))\n",
    "\n",
    "chars = set(text)\n",
    "print(\"total chars:\", len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Divide the text into partially dependent character sequences of length equal to maxlen\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print(\"nb sequences:\", len(sentences))\n",
    "\n",
    "# Convert text to numerical vectors\n",
    "print(\"Vectorization...\")\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        \n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# Create a model\n",
    "print(\"Building model...\")\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.LSTM(512, return_sequences=False))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(len(chars), activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# Auxilary function for gaining index from an array of probabilities\n",
    "def sample(a, temperature=1.0):  \n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    a = a / np.sum(a)\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "# Train model and generate output after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Iteration\", iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print(\"----- diversity:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print(\"----- Generating with seed: \\\"\" + sentence + \"\\\"\")\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for _ in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skript si samozřejmě můžete pustit, ale trénování poběží neskutečně dlouho. Proto jsme skript pustili na Google Colab a na výsledky se můžete podívat [zde](https://colab.research.google.com/drive/1B7zys275xmpPqahPwNvuYMPLmgvlV3l5) nebo v souboru [*results.txt*](results.txt)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
